{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIYm2iAIzZ9Z"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxADIknt5biF"
   },
   "source": [
    "The code for the LLM Agent data generation can be found [here](https://codeshare.io/Q8qyBL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUJFxpRxzo9b"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_4ufcoXuW66"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "! pip install torch_geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, BatchNorm, LayerNorm, DenseGCNConv, global_mean_pool, dense_diff_pool\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfK-oH493IC5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQw2IoWlzj-_"
   },
   "source": [
    "## Setting Random Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwUHgWE9uaUv"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfaaLEY8zwjh"
   },
   "source": [
    "# Reading in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiSXeYlyz3sw"
   },
   "source": [
    "## Data Downloaded Directly from ARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzQoNFaTMGOK"
   },
   "outputs": [],
   "source": [
    "root = \"/content/drive/MyDrive/CS224W_Project/\"\n",
    "train_path = root + \"ARC-800-tasks/training/\"\n",
    "val_path = root + \"ARC-800-tasks/evaluation/\" # not used for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMK_ZNdCUVkn"
   },
   "outputs": [],
   "source": [
    "# length 400, each element is a list of varying length, depending on how many training examples there are\n",
    "# for one pattern (usually 3-5)\n",
    "inputs_all = []\n",
    "\n",
    "# same shape as inputs_all, but stores the ground-truths (the solutions to the puzzles)\n",
    "outputs_all = []\n",
    "\n",
    "# length 400, each element is an input for the LLM to solve and\n",
    "# FOLLOWS THE SAME PATTERN as the inputs in the corresponding element of inputs_all\n",
    "X_all = []\n",
    "\n",
    "# length 400, each element stores the corresponding ground-truths in X_all that the LLM won't see\n",
    "y_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vHBWm9nMSbz"
   },
   "outputs": [],
   "source": [
    "for i in os.listdir(train_path):\n",
    "  # print(i)\n",
    "  with open(train_path + i, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    inputs = [sample[\"input\"] for sample in data[\"train\"]]\n",
    "    outputs = [sample[\"output\"] for sample in data[\"train\"]]\n",
    "    X = data[\"test\"][0][\"input\"]\n",
    "    y = data[\"test\"][0][\"output\"]\n",
    "    inputs_all.append(inputs)\n",
    "    outputs_all.append(outputs)\n",
    "    X_all.append(X)\n",
    "    y_all.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sda5TmD_TV8"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "  if i < len(axes[0]) - 1:\n",
    "    ax.imshow(inputs_all[1][i], cmap=\"Greys\")\n",
    "    ax.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    ax.set_title(f\"Input {i+1}\")\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "  if i < len(axes[1]) - 1:\n",
    "    ax.imshow(outputs_all[1][i], cmap=\"Greys\")\n",
    "    ax.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    ax.set_title(f\"Output {i+1}\")\n",
    "\n",
    "axes[0,3].imshow(X_all[1], cmap=\"Greys\")\n",
    "axes[0,3].tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "axes[0,3].set_title(\"X\")\n",
    "\n",
    "grid = np.zeros((10, 10))\n",
    "pattern = [(1, 3), (2, 2), (1, 4), (1, 5), (2, 6), (3, 6), (4, 5), (5, 4), (6, 4), (8, 4)]\n",
    "for x, y in pattern:\n",
    "    grid[x, y] = 1\n",
    "\n",
    "axes[1,3].imshow(grid, cmap=\"Reds\")\n",
    "axes[1,3].tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "axes[1,3].set_title(\"Y\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"input_output_example.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlROaTj0-eD4"
   },
   "outputs": [],
   "source": [
    "fig,\n",
    "plt.imshow(inputs_all[0][0], cmap=\"Greys\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT0Y6IJJz8DB"
   },
   "source": [
    "## Data Outputs from LLM Agents\n",
    "\n",
    "Note: Code for LLM Agent data generation is in another notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_SDu4NpEVWo"
   },
   "outputs": [],
   "source": [
    "all_graphs = []\n",
    "all_files = os.listdir(root + \"AgentIdeas/\")\n",
    "all_files = sorted(all_files, key=lambda x: float(x[x.index(\"_\")+1:x.rindex(\"_\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kV8LbYZWzVVM"
   },
   "outputs": [],
   "source": [
    "num_iterations = 6 # From Krish's data generation\n",
    "root = \"/content/drive/MyDrive/CS224W_Project/\"\n",
    "for i in all_files:\n",
    "  with open(root + \"AgentIdeas/\" + i, \"r\") as file:\n",
    "    temp = []\n",
    "    data = json.load(file)\n",
    "    for j in range(len(data[\"embeddings_data\"])): # number of agents * num_iterations\n",
    "      print(data[\"embeddings_data\"][j][\"metadata\"][\"agent_persona\"], \"idea\", j % num_iterations, \"for problem\",\n",
    "            data[\"embeddings_data\"][j][\"metadata\"][\"problem_idx\"])\n",
    "      temp.append(data[\"embeddings_data\"][j][\"embedding\"])\n",
    "    all_graphs.append(np.array(temp))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BD86RcMho6Sr"
   },
   "outputs": [],
   "source": [
    "data_array = np.array(all_graphs)\n",
    "num_graphs, num_nodes, embedding_dim = np.array(all_graphs).shape\n",
    "\n",
    "# Dimensions: [number of ARC problems, number of ideas (nodes), node feature dimension]\n",
    "data_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZoMoeaN6-zu"
   },
   "source": [
    "# Graph Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjKyJcFa0LpX"
   },
   "source": [
    "## Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GXMgMPHlCLK"
   },
   "outputs": [],
   "source": [
    "def create_kmeans_edges(node_features, n_clusters=5):\n",
    "  kmeans = KMeans(n_clusters=n_clusters)\n",
    "  clusters = kmeans.fit_predict(node_features)\n",
    "  edges = []\n",
    "  for i in range(len(node_features)):\n",
    "    same_cluster_nodes = np.where(clusters == clusters[i])[0]\n",
    "    edges.extend([(i, j) for j in same_cluster_nodes if i != j])\n",
    "  return edges\n",
    "\n",
    "def create_similarity_edges(node_features, threshold=0.7):\n",
    "  sim_matrix = cosine_similarity(node_features)\n",
    "  idxs = np.triu_indices_from(sim_matrix, k=1)\n",
    "  upper_tri_values = sim_matrix[idxs]\n",
    "  valid_pairs = np.where(upper_tri_values > threshold)\n",
    "  edges = list(zip(idxs[0][valid_pairs], idxs[1][valid_pairs]))\n",
    "  return edges\n",
    "\n",
    "def create_diffusion_edges(node_features, temp=0.1, random_prob=0.1):\n",
    "  distances = pdist(node_features)\n",
    "  dist_matrix = squareform(distances)\n",
    "  diff_matrix = np.exp(-dist_matrix / temp)\n",
    "  mean_diff = np.mean(diff_matrix)\n",
    "  idxs = np.triu_indices_from(diff_matrix, k=1)\n",
    "  valid_pairs = np.where(diff_matrix[idxs] > mean_diff)\n",
    "  edges = set(zip(idxs[0][valid_pairs], idxs[1][valid_pairs]))\n",
    "  n_nodes = len(node_features)\n",
    "  n_random = int(random_prob * n_nodes * (n_nodes - 1) / 2)\n",
    "  all_possible = np.array(list(zip(*idxs)))\n",
    "  random_indices = np.random.choice(all_possible.shape[0], size=n_random, replace=False)\n",
    "  random_edges = all_possible[random_indices]\n",
    "  edges.update(map(tuple, random_edges))\n",
    "  return list(edges)\n",
    "\n",
    "def combine_edges(node_features, k=3, sim_threshold=0.82, temp=0.1, random_prob=0.1):\n",
    "  edges = set()\n",
    "  edges.update(create_kmeans_edges(node_features, k))\n",
    "  edges.update(create_similarity_edges(node_features, sim_threshold))\n",
    "  edges.update(create_diffusion_edges(node_features, temp, random_prob))\n",
    "  return list(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqCb__RulMVf"
   },
   "outputs": [],
   "source": [
    "def create_graph_data_from_numpy(numpy_array, labels):\n",
    "  dataset = []\n",
    "  num_graphs, num_nodes, embedding_dim = numpy_array.shape\n",
    "  for i in range(num_graphs):\n",
    "    node_features = torch.tensor(numpy_array[i], dtype=torch.float)\n",
    "    edges = combine_edges(numpy_array[i])\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    graph_label = torch.tensor(labels[i], dtype=torch.float)\n",
    "    data = Data(x=node_features, edge_index=edge_index, y=graph_label)\n",
    "    dataset.append(data)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idmXrqQIl63t"
   },
   "outputs": [],
   "source": [
    "graph_dataset = create_graph_data_from_numpy(data_array, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AElTt0jWcoRh"
   },
   "outputs": [],
   "source": [
    "# We can either construct the edges using a threshold cosine similarity\n",
    "\n",
    "def create_graph_data_from_numpy(numpy_array, labels, threshold=0.5):\n",
    "    dataset = []\n",
    "    num_graphs, num_nodes, embedding_dim = numpy_array.shape\n",
    "    for i in range(num_graphs):\n",
    "        node_features = torch.tensor(numpy_array[i], dtype=torch.float)\n",
    "        similarity_matrix = cosine_similarity(numpy_array[i])\n",
    "        adjacency_matrix = (similarity_matrix > threshold).astype(int)\n",
    "        np.fill_diagonal(adjacency_matrix, 0)\n",
    "        edge_index = np.array(np.nonzero(adjacency_matrix))\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "        graph_label = torch.tensor(labels[i], dtype=torch.float)\n",
    "        graph_data.y = graph_label\n",
    "        dataset.append(graph_data)\n",
    "    return dataset\n",
    "\n",
    "graph_dataset = create_graph_data_from_numpy(data_array, y_all, threshold=0.82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5f0aHD-QSgS"
   },
   "outputs": [],
   "source": [
    "# Or we can construct the edges using k-nearest neighbors\n",
    "\n",
    "def create_graph_data_from_numpy(numpy_array, labels, k=3):\n",
    "  dataset = []\n",
    "  num_graphs, num_nodes, embedding_dim = numpy_array.shape\n",
    "  for i in range(num_graphs):\n",
    "    node_features = torch.tensor(numpy_array[i], dtype=torch.float)\n",
    "    adjacency_matrix = kneighbors_graph(numpy_array[i], n_neighbors=k, mode=\"connectivity\", include_self=False)\n",
    "    edge_index = torch.tensor(np.array(adjacency_matrix.nonzero()), dtype=torch.long)\n",
    "    graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "    graph_label = torch.tensor(labels[i], dtype=torch.float)\n",
    "    graph_data.y = graph_label\n",
    "    dataset.append(graph_data)\n",
    "  return dataset\n",
    "\n",
    "graph_dataset = create_graph_data_from_numpy(data_array, y_all, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ujo1dynK-sKv"
   },
   "outputs": [],
   "source": [
    "# Visualize one graph\n",
    "idx = np.random.randint(len(graph_dataset))\n",
    "graph_data = graph_dataset[idx]\n",
    "num_nodes = graph_data.x.shape[0]\n",
    "\n",
    "G = nx.Graph()\n",
    "for i in range(num_nodes):\n",
    "  G.add_node(i, feature=graph_data.x[i].numpy())\n",
    "\n",
    "edge_index = graph_data.edge_index.numpy()\n",
    "for i in range(edge_index.shape[1]):\n",
    "    u, v = edge_index[:,i]\n",
    "    G.add_edge(u, v)\n",
    "\n",
    "pos = nx.spring_layout(G, seed=0)\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "# plt.savefig(\"example_graph\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-JPYloOclbh"
   },
   "outputs": [],
   "source": [
    "def pad_example(matrix, desired_shape=(10,10)):\n",
    "  r = matrix.shape[0]\n",
    "  c = matrix.shape[1]\n",
    "\n",
    "  r_needed, c_needed = desired_shape\n",
    "  top_pad = (r_needed - r) // 2\n",
    "  left_pad = (c_needed - c) // 2\n",
    "\n",
    "  data = np.zeros(desired_shape)\n",
    "  data[top_pad:top_pad+r, left_pad:left_pad+c] = matrix\n",
    "  return data\n",
    "\n",
    "# LLM output embeddings are 10 x 10, but some ARC answers are larger in size\n",
    "# so for simplicity's sake we just select the ones that are equal or smaller\n",
    "# in size and pad them if needed\n",
    "graph_dataset = [i for i in graph_dataset if i.y.shape[0] <= 10 and i.y.shape[1] <= 10]\n",
    "\n",
    "# Normalize labels because LLM embeddings are normalized\n",
    "for i in graph_dataset:\n",
    "  i.y = torch.tensor(pad_example(i.y.numpy()), dtype=torch.float32).view(1,-1) / torch.max(i.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eByI8QhrSDgD"
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(graph_dataset))\n",
    "val_size = len(graph_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(graph_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJHVa6y-56bH"
   },
   "source": [
    "## Graph Convolutional Network and Graph Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUmJDqjm1jnU"
   },
   "outputs": [],
   "source": [
    "class ResGCNBlock(nn.Module):\n",
    "  def __init__(self, in_c, out_c):\n",
    "    super(ResGCNBlock, self).__init__()\n",
    "    self.conv = GCNConv(in_c, out_c)\n",
    "    self.norm = BatchNorm(out_c)\n",
    "    self.shortcut = (nn.Linear(in_c, out_c) if in_c != out_c else nn.Identity())\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    identity = self.shortcut(x)\n",
    "    out = self.conv(x, edge_index)\n",
    "    out = self.norm(out)\n",
    "    return F.relu(out + identity)\n",
    "\n",
    "class GraphConvolutionalModel(nn.Module):\n",
    "  def __init__(self, num_features, hidden_dim, output_dim, num_layers=3):\n",
    "    super(GraphConvolutionalModel, self).__init__()\n",
    "    self.layers = nn.ModuleList()\n",
    "\n",
    "    # ResGCNBlock modules\n",
    "    self.layers.append(ResGCNBlock(num_features, hidden_dim))\n",
    "    for i in range(num_layers - 2):\n",
    "      self.layers.append(ResGCNBlock(hidden_dim, hidden_dim))\n",
    "    self.layers.append(ResGCNBlock(hidden_dim, hidden_dim))\n",
    "\n",
    "    # Fully connected tail\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, output_dim)\n",
    "    )\n",
    "\n",
    "  def forward(self, data):\n",
    "    x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, edge_index)\n",
    "    x = global_mean_pool(x, batch)\n",
    "    graph_embedding = self.fc(x)\n",
    "    return graph_embedding\n",
    "\n",
    "num_node_features = data_array.shape[2] # 100\n",
    "hidden_dim = 64\n",
    "embedding_dim = data_array.shape[2] # 100\n",
    "\n",
    "model = GraphConvolutionalModel(num_node_features, hidden_dim, embedding_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCdo2g1hkzl0"
   },
   "outputs": [],
   "source": [
    "class MultiHeadGAT(nn.Module):\n",
    "  def __init__(self, num_features, hidden_dim, output_dim, num_heads=4):\n",
    "    super().__init__()\n",
    "    self.gat1 = GATConv(num_features, hidden_dim, heads=num_heads)\n",
    "    self.gat2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1)\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, output_dim)\n",
    "    )\n",
    "  def forward(self, data):\n",
    "    x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "    x = F.relu(self.gat1(x, edge_index))\n",
    "    x = F.relu(self.gat2(x, edge_index))\n",
    "    x = global_mean_pool(x, batch)\n",
    "    return self.fc(x)\n",
    "\n",
    "num_features = data_array.shape[2]\n",
    "hidden_dim = 64\n",
    "output_dim = data_array.shape[2]\n",
    "\n",
    "model = MultiHeadGAT(num_features, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4F-eFdwaSxqU"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_epoch(loader, model, optimizer):\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  for data in loader:\n",
    "    optimizer.zero_grad()\n",
    "    embeddings = model(data)\n",
    "    loss = F.mse_loss(embeddings, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss.item()\n",
    "  return epoch_loss / len(loader)\n",
    "\n",
    "def validate(loader, model):\n",
    "  model.eval()\n",
    "  epoch_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      embeddings = model(data)\n",
    "      loss = F.mse_loss(embeddings, data.y)\n",
    "      epoch_loss += loss.item()\n",
    "  return epoch_loss / len(loader)\n",
    "\n",
    "num_epochs = 400\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  train_loss = train_epoch(train_loader, model, optimizer)\n",
    "  val_loss = validate(val_loader, model)\n",
    "  print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJHN4aSSksXv"
   },
   "outputs": [],
   "source": [
    "# Collect all predictions\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for i in train_loader:\n",
    "  preds = model(i).detach().numpy()\n",
    "  labels = i.y\n",
    "  all_preds.append(preds)\n",
    "  all_labels.append(labels)\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZsEJ1chEJRk"
   },
   "outputs": [],
   "source": [
    "# Visualizing Performance\n",
    "def apply_fourier(img):\n",
    "  f = np.fft.fft2(img.reshape(10, 10))\n",
    "  fshift = np.fft.fftshift(f)\n",
    "  magnitude_spectrum = np.log(np.abs(fshift))\n",
    "\n",
    "  rows, cols = img.reshape(10, 10).shape\n",
    "  crow, ccol = rows // 2, cols // 2\n",
    "  mask = np.zeros((rows, cols))\n",
    "  mask[crow-30:crow+30, ccol-30:ccol+30] = 1\n",
    "\n",
    "  fshift = fshift * mask\n",
    "  f_ishift = np.fft.ifftshift(fshift)\n",
    "  return np.abs(np.fft.ifft2(f_ishift))\n",
    "\n",
    "num_examples = 10\n",
    "columns = 5\n",
    "rows = (num_examples + columns - 1) // columns\n",
    "fig, axes = plt.subplots(2 * rows, columns, figsize=(columns * 2, rows * 3))\n",
    "for i in range(num_examples):\n",
    "  row, col = divmod(i, columns)\n",
    "  axes[2*row, col].imshow(apply_fourier(preds[i]), cmap=\"gray\")\n",
    "  axes[2*row, col].set_title(f\"Predicted #{i+1}\")\n",
    "  axes[2*row, col].axis(\"off\")\n",
    "  axes[2*row+1, col].imshow(labels[i].reshape(10, 10), cmap=\"gray\")\n",
    "  axes[2*row+1, col].set_title(f\"True #{i+1}\")\n",
    "  axes[2*row+1, col].axis(\"off\")\n",
    "\n",
    "for ax in axes.flat[num_examples:]:\n",
    "  ax.axis(\"off\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.0, hspace=0.3)\n",
    "# plt.savefig(\"test.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4HAO00O6B_d"
   },
   "source": [
    "## Use PyTorch Hooks to Analyze GCN Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEQTDZ5PlJUE"
   },
   "outputs": [],
   "source": [
    "class GCNVisualizer(GraphConvolutionalModel):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.activations = []\n",
    "    for conv in self.convs:\n",
    "      conv.register_forward_hook(self.save_activations)\n",
    "  def save_activations(self, module, input, output):\n",
    "    self.activations.append(output)\n",
    "  def reset_activations(self):\n",
    "    self.activations = []\n",
    "\n",
    "visualizer = GCNVisualizer(num_node_features=100, hidden_dim=64, embedding_dim=100, num_layers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWMoO31_wscc"
   },
   "source": [
    "### For one example, test that GCNVisualizer works, and plot node activations, collapsing by hidden dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIMSFqaFwu8F"
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "visualizer.reset_activations()\n",
    "visualizer(graph_dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtwQndheSMIX"
   },
   "outputs": [],
   "source": [
    "mean_activations = np.mean(visualizer.activations[0].detach().numpy(), axis=1)\n",
    "data = pd.DataFrame({\"Node Index\": range(len(mean_activations)), \"Mean Activations\": mean_activations})\n",
    "data_melted = data.melt(id_vars=[\"Node Index\"], value_vars=[\"Mean Activations\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=data_melted, x=\"Node Index\", y=\"Value\", hue=\"Metric\", palette=\"muted\")\n",
    "plt.title(\"Mean of Node Activations\")\n",
    "plt.xlabel(\"Node Index\")\n",
    "plt.ylabel(\"Activation Value\")\n",
    "# plt.savefig(\"mean_activation_per_node.png\", dpi=200)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKLDJvj6lv69"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(visualizer.activations[0].detach().numpy())\n",
    "data_melted = data.melt(var_name=\"Node Index\", value_name=\"Activation Value\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=data.T, orient=\"h\", palette=\"muted\", showmeans=True)\n",
    "\n",
    "plt.title(\"Activation Distributions per Node\")\n",
    "plt.xlabel(\"Activation Value\")\n",
    "plt.ylabel(\"Node Index\")\n",
    "# plt.savefig(\"activation_per_node.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-r8Zwt4JnVmR"
   },
   "outputs": [],
   "source": [
    "def plot_node_activations(activations, labels=None):\n",
    "  tsne = TSNE(n_components=2, perplexity=29)\n",
    "  reduced_activations = tsne.fit_transform(activations)\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  scatter = plt.scatter(reduced_activations[:, 0], reduced_activations[:, 1], c=labels, cmap='Spectral', alpha=0.7)\n",
    "  if labels is not None:\n",
    "    plt.colorbar(scatter)\n",
    "  plt.title(\"Node Activations\")\n",
    "  plt.show()\n",
    "\n",
    "node_labels = [0] * 6 + [1] * 6 + [2] * 6 + [3] * 6 + [4] * 6\n",
    "plot_node_activations(visualizer.activations[2].detach().cpu().numpy(), labels=node_labels)\n",
    "\n",
    "# assign clusters as labels\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "clusters = kmeans.fit_predict(visualizer.activations[2].detach().cpu().numpy())\n",
    "plot_node_activations(visualizer.activations[2].detach().cpu().numpy(), labels=clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfSDci0Ixlmc"
   },
   "source": [
    "## Reasoning Framework\n",
    "We can interpret the activation values from GCN and determine node importance (LLM agent importance), ARC problem similarity, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSZh4Gz26UNN"
   },
   "outputs": [],
   "source": [
    "def calculate_mean_activations(dataset):\n",
    "  # one entry for each layer of the model (three total)\n",
    "  all_mean_activations = {\"1\": [], \"2\": [], \"3\": []}\n",
    "  for i in range(len(dataset)):\n",
    "    visualizer.reset_activations()\n",
    "    _ = visualizer(dataset[i])\n",
    "    for j in range(len(visualizer.activations)):\n",
    "      mean_activations = np.mean(visualizer.activations[0].detach().numpy(), axis=1)\n",
    "      all_mean_activations[str(j+1)].append(mean_activations)\n",
    "  return all_mean_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbkiDVSfvI34"
   },
   "outputs": [],
   "source": [
    "train_mean_activations = calculate_mean_activations(train_dataset)\n",
    "val_mean_activations = calculate_mean_activations(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qu1oxAPyvkgW"
   },
   "outputs": [],
   "source": [
    "def plot_activations(activations, normalized=True, save=False, title=None):\n",
    "  sns.set(font_scale=0.6)\n",
    "  if normalized:\n",
    "    fig = sns.clustermap(np.array(activations[\"3\"]) == np.max(np.array(activations[\"3\"]), axis=1, keepdims=True),\n",
    "                        cmap=\"Blues\", figsize=(20,20))\n",
    "  else:\n",
    "    fig = sns.clustermap(np.array(activations[\"3\"]))\n",
    "  fig.ax_heatmap.tick_params(axis='x', labelsize=10, width=0.5)  # Smaller x-axis labels\n",
    "  fig.ax_heatmap.set_xlabel(\"Node\")\n",
    "  fig.ax_heatmap.set_ylabel(\"ARC Problem Number\")\n",
    "  if save:\n",
    "    fig.fig.savefig(title, dpi=300, bbox_inches=\"tight\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kyow__4Hwgs2"
   },
   "outputs": [],
   "source": [
    "plot_activations(train_mean_activations, normalized=True, save=True, title=\"train_max_activations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gj2X7lxQwkKd"
   },
   "outputs": [],
   "source": [
    "plot_activations(val_mean_activations, normalized=True, save=True, title=\"val_max_activations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqlp6lgavzqZ"
   },
   "outputs": [],
   "source": [
    "plot_activations(train_mean_activations, normalized=False, save=True, title=\"train_activations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuA6qgo7nReT"
   },
   "outputs": [],
   "source": [
    "plot_activations(val_mean_activations, normalized=False, save=True, title=\"val_activations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcHY7_Wupq-x"
   },
   "source": [
    "## DiffPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTNPGzpuTb82"
   },
   "outputs": [],
   "source": [
    "# DiffPool needs a different data set structure and data loader\n",
    "# Adjacency matrices need to be dense and node features need to have an extra dimension\n",
    "\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, graph_dataset):\n",
    "    self.graphs_and_labels = graph_dataset\n",
    "  def __len__(self):\n",
    "    return len(self.graphs_and_labels)\n",
    "  def __getitem__(self, idx):\n",
    "    graph = self.graphs_and_labels[idx]\n",
    "    # Convert to dense format\n",
    "    x = graph.x\n",
    "    y = graph.y\n",
    "    adj = to_dense_adj(graph.edge_index, batch=None, max_num_nodes=30).squeeze(0)\n",
    "    return x, adj, y\n",
    "\n",
    "def collate(batch):\n",
    "    x_list, adj_list, label_list = zip(*batch)\n",
    "    x_batch = torch.stack(x_list)  # Shape:[batch_size, num_nodes, feature_dim]\n",
    "    adj_batch = torch.stack(adj_list)  # Shape: [batch_size, num_nodes, num_nodes]\n",
    "    labels = torch.tensor(label_list, dtype=torch.long)\n",
    "    return x_batch, adj_batch, labels\n",
    "\n",
    "diffpool_dataset = GraphDataset(graph_dataset)\n",
    "diffpool_loader = DataLoader(diffpool_dataset, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsgQbl64Q5Y7"
   },
   "outputs": [],
   "source": [
    "class DiffPoolGCN(nn.Module):\n",
    "  def __init__(self, num_node_features, hidden_dim, embedding_dim, num_clusters_list):\n",
    "    super(DiffPoolGCN, self).__init__()\n",
    "    self.num_layers = len(num_clusters_list)\n",
    "    self.hidden_dim = hidden_dim\n",
    "\n",
    "    # GCNConv layers for message passing\n",
    "    self.gcns_embed = nn.ModuleList([DenseGCNConv(num_node_features if i == 0 else hidden_dim, hidden_dim)\n",
    "                                     for i in range(self.num_layers)])\n",
    "\n",
    "    # GCNConv layers for cluster assignment\n",
    "    self.gcns_assign = nn.ModuleList([DenseGCNConv(hidden_dim, num_clusters_list[i]) for i in range(self.num_layers)])\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(hidden_dim * self.num_layers, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, embedding_dim)\n",
    "    )\n",
    "\n",
    "  def forward(self, x_batch, adj_batch):\n",
    "    all_embeds = []\n",
    "    s_matrices = []\n",
    "    total_loss = 0\n",
    "    for i in range(self.num_layers):\n",
    "      x_batch = self.gcns_embed[i](x_batch, adj_batch).relu()\n",
    "\n",
    "      # Cluster assignment\n",
    "      s = self.gcns_assign[i](x_batch, adj_batch)\n",
    "      s = torch.softmax(s, dim=-1)\n",
    "      s_matrices.append(s)\n",
    "\n",
    "      x_batch, adj_batch, _, entropy_reg = dense_diff_pool(x_batch, adj_batch, s)\n",
    "      total_loss += entropy_reg\n",
    "\n",
    "      # Graph-level representation\n",
    "      all_embeds.append(torch.mean(x_batch, dim=1))\n",
    "    graph_embedding = torch.cat(all_embeds, dim=-1)\n",
    "    graph_embedding = self.fc(graph_embedding)\n",
    "    return graph_embedding, total_loss, s_matrices\n",
    "\n",
    "num_node_features = data_array.shape[2]\n",
    "hidden_dim = 64\n",
    "embedding_dim = data_array.shape[2]\n",
    "num_clusters_list = [15, 5]\n",
    "\n",
    "model = DiffPoolGCN(num_node_features=num_node_features, hidden_dim=hidden_dim, embedding_dim=embedding_dim, num_clusters_list=num_clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipbcewFZhowj"
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(diffpool_dataset))\n",
    "val_size = len(diffpool_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(diffpool_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2_0evefTM-E"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "for epoch in range(300):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  for x_batch, adj_batch, labels in train_dataset:\n",
    "    optimizer.zero_grad()\n",
    "    graph_embeddings, diffpool_loss, _ = model(x_batch, adj_batch)\n",
    "    objective_loss = F.mse_loss(graph_embeddings, labels.squeeze(1))\n",
    "    loss = objective_loss + 0.1 * diffpool_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    val_loss = 0\n",
    "    for x_batch, adj_batch, labels in val_dataset:\n",
    "      graph_embeddings, diffpool_loss, _ = model(x_batch, adj_batch)\n",
    "      objective_loss = F.mse_loss(graph_embeddings, labels.squeeze(1))\n",
    "      loss = objective_loss + 0.1 * diffpool_loss\n",
    "      val_loss += diffpool_loss.item()\n",
    "\n",
    "  print(f\"Epoch {epoch + 1}, Train Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYaWGXjsQEVw"
   },
   "outputs": [],
   "source": [
    "x_batch, adj_batch, label_batch = next(iter(diffpool_loader))\n",
    "model.eval()\n",
    "\n",
    "graph_embedding, total_loss, s_matrices = model(x_batch, adj_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2p1vO20TBR4"
   },
   "outputs": [],
   "source": [
    "s_matrix = s_matrices[0]\n",
    "\n",
    "graph_idx = 5\n",
    "s_single = s_matrix[graph_idx].detach().cpu().numpy() # [num_nodes, num_clusters]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(s_single, annot=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CUJFxpRxzo9b",
    "zQw2IoWlzj-_",
    "PjKyJcFa0LpX",
    "wJHVa6y-56bH",
    "JfSDci0Ixlmc",
    "YcHY7_Wupq-x"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
